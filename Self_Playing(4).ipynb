{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from game import Game, GameState\n",
    "from agent import Agent\n",
    "from memory import Memory\n",
    "from model import Residual_CNN\n",
    "from funcs import playMatches, playMatchesBetweenVersions\n",
    "\n",
    "import loggers as lg\n",
    "\n",
    "from settings import run_folder, run_archive_folder\n",
    "import initialise\n",
    "import pickle\n",
    "import config\n",
    "\n",
    "import time\n",
    "import os\n",
    "import fileflag\n",
    "\n",
    "env = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From d:\\Google 드라이브(ajou)\\캡스톤디자인\\DeepReinforceLearning\\DeepReinforcementLearning\\loss.py:13: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From d:\\Google 드라이브(ajou)\\캡스톤디자인\\DeepReinforceLearning\\DeepReinforcementLearning\\loss.py:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nFile flag 2 ON\nFile flag 2 OFF\nUSING TEMP MODEL ...\nITERATION NUMBER 1\nBEST PLAYER VERSION temp\nSELF PLAYING 10 EPISODES...\n1"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8afad010f1ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SELF PLAYING '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' EPISODES...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start self playing...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayMatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_player\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_player\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger_main\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mturns_until_tau0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTURNS_UNTIL_TAU0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Google 드라이브(ajou)\\캡스톤디자인\\DeepReinforceLearning\\DeepReinforcementLearning\\funcs.py\u001b[0m in \u001b[0;36mplayMatches\u001b[1;34m(player1, player2, EPISODES, logger, turns_until_tau0, memory, goes_first)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m#### Run the MCTS algo and return an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mturn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mturns_until_tau0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMCTS_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayerTurn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'agent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMCTS_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNN_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayerTurn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'agent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Google 드라이브(ajou)\\캡스톤디자인\\DeepReinforceLearning\\DeepReinforcementLearning\\agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, tau)\u001b[0m\n\u001b[0;32m     79\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuildMCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchangeRootMCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m#### run the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Google 드라이브(ajou)\\캡스톤디자인\\DeepReinforceLearning\\DeepReinforcementLearning\\agent.py\u001b[0m in \u001b[0;36mchangeRootMCTS\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[1;31m# self.mcts.root = self.mcts.tree[state.id]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[0mtemp_mcts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpuct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodeid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecentnodeid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mnodeid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 최근 방문한 노드로 기록되었는데 트리에 없는 문제때문에 if문 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                                 \u001b[0mtemp_mcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnodeid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "no = 4 # Selfplay 파일 여러개 중 이 파일의 번호\n",
    "\n",
    "######## CREATE THE PLAYERS ########\n",
    "### Temp_memory 있는지 꼭 확인\n",
    "\n",
    "# 빈 Neural Network 생성\n",
    "current_NN = Residual_CNN(config.REG_CONST, config.LEARNING_RATE, (2,) + env.grid_shape,   env.action_size, config.HIDDEN_CNN_LAYERS)   # 빈 NN\n",
    "best_NN = Residual_CNN(config.REG_CONST, config.LEARNING_RATE, (2,) +  env.grid_shape,   env.action_size, config.HIDDEN_CNN_LAYERS)     # 사용하는 NN\n",
    "\n",
    "# Best Player Model 불러옴\n",
    "if os.path.exists('./run/models/model_temp.h5'):    # model_temp 사용\n",
    "    best_player_version = 'temp'\n",
    "    fileflag.fileFlag_on(2)\n",
    "    m_tmp = best_NN.read_tmp_selfplay(env.name)\n",
    "    fileflag.fileFlag_off(2)\n",
    "    best_NN.model.set_weights(m_tmp.get_weights())\n",
    "    print('USING TEMP MODEL ...')\n",
    "elif initialise.INITIAL_MODEL_VERSION != None:      # 지정 버전 모델 사용\n",
    "    best_player_version  = initialise.INITIAL_MODEL_VERSION\n",
    "    print('LOADING MODEL VERSION ' + str(initialise.INITIAL_MODEL_VERSION) + '...')\n",
    "    m_tmp = best_NN.read(env.name, initialise.INITIAL_RUN_NUMBER, best_player_version)\n",
    "    best_NN.model.set_weights(m_tmp.get_weights())\n",
    "else:                                               # 빈 모델 사용\n",
    "    best_player_version = 0\n",
    "    best_NN.model.set_weights(current_NN.model.get_weights())\n",
    "    print('USING EMPTY MODEL ...')\n",
    "\n",
    "\n",
    "######## CREATE THE PLAYER ########\n",
    "best_player = Agent('best_player', env.state_size, env.action_size, config.MCTS_SIMS, config.CPUCT, best_NN)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "first_play = True\n",
    "\n",
    "\n",
    "while 1:\n",
    "    lg.logger_test.info('Self Playing module ' + str(no) +' alive')\n",
    "    memory = Memory(config.MEMORY_SIZE)\n",
    "        \n",
    "    iteration += 1\n",
    "    reload(lg)\n",
    "    reload(config)\n",
    "    \n",
    "    print('ITERATION NUMBER ' + str(iteration))\n",
    "    \n",
    "    lg.logger_main.info('BEST PLAYER VERSION: %s', str(best_player_version))\n",
    "    print('BEST PLAYER VERSION ' + str(best_player_version))\n",
    "\n",
    "    ######## SELF PLAY ########\n",
    "    print('SELF PLAYING ' + str(config.EPISODES) + ' EPISODES...')\n",
    "    lg.logger_test.info('Start self playing...')\n",
    "    _, memory, _, _ = playMatches(best_player, best_player, config.EPISODES, lg.logger_main, turns_until_tau0 = config.TURNS_UNTIL_TAU0, memory = memory)\n",
    "    print('\\n')\n",
    "    \n",
    "    memory.clear_stmemory()\n",
    "\n",
    "    ######## LOAD OLD MEMORIES & DUMP ########\n",
    "    #if first_play == True:\n",
    "    #    if initialise.INITIAL_MEMORY_VERSION != None:\n",
    "    #        memory_old = pickle.load( open( \"./run/memory/memory\" + '_' + str(initialise.INITIAL_MEMORY_VERSION) + \".p\",   \"rb\" ) )\n",
    "    #        print('USING MEMORY : ' + str(initialise.INITIAL_MEMORY_VERSION))\n",
    "    #        print('INITIAL MEMORY SIZE: ' + str(len(memory_old.ltmemory)))\n",
    "    #    else:\n",
    "    #        memory_old = Memory(config.MEMORY_SIZE)\n",
    "    #        print('NEW TEMP MEMORY')\n",
    "    \n",
    "    fileflag.fileFlag_on(0)\n",
    "    memory_old = pickle.load( open( \"./run/memory/memory_temp\" + \".p\",   \"rb\" ) )\n",
    "    fileflag.fileFlag_off(0)\n",
    "    print('USING TEMP MEMORY')\n",
    "    print('TEMP MEMORY SIZE: ' + str(len(memory_old.ltmemory)))\n",
    "\n",
    "    first_play = False\n",
    "\n",
    "    for i in memory.ltmemory:\n",
    "\t    memory_old.ltmemory.append(i)\n",
    "    \n",
    "    print('NEW MEMORY SIZE: ' + str(len(memory.ltmemory)))\n",
    "\n",
    "    fileflag.fileFlag_on(0)\n",
    "    pickle.dump( memory_old, open( \"./run/memory/memory_temp.p\", \"wb\" ) )\n",
    "    fileflag.fileFlag_off(0)\n",
    "    if (iteration + no) % 5 == 0:\n",
    "        pickle.dump( memory_old, open( \"./run/memory/memory_\" + time.strftime( '%m%d-%H%M%S',time.localtime(time.time()) ) + '_no' + str(no) + \".p\", \"wb\" ) )\n",
    "\n",
    "    lg.logger_test.info('Self Play memory dumped')\n",
    "\n",
    "\n",
    "    # MODEL 교체\n",
    "    if os.path.exists('./run/models/model_temp.h5'):    # model_temp 사용\n",
    "        best_player_version = 0\n",
    "        fileflag.fileFlag_on(2)\n",
    "        m_tmp = best_NN.read_tmp_selfplay(env.name)\n",
    "        fileflag.fileFlag_off(2)\n",
    "        best_NN.model.set_weights(m_tmp.get_weights())\n",
    "        print('LOADING TEMP MODEL ...')\n",
    "    elif initialise.INITIAL_MODEL_VERSION != None:      # 지정 버전 모델 사용\n",
    "        best_player_version  = initialise.INITIAL_MODEL_VERSION\n",
    "        print('LOADING MODEL VERSION ' + str(initialise.INITIAL_MODEL_VERSION) + '...')\n",
    "        m_tmp = best_NN.read(env.name, initialise.INITIAL_RUN_NUMBER, best_player_version)\n",
    "        best_NN.model.set_weights(m_tmp.get_weights())\n",
    "    else:                                               # 빈 모델 사용\n",
    "        best_player_version = 0\n",
    "        best_NN.model.set_weights(current_NN.model.get_weights())\n",
    "        print('LOADING EMPTY MODEL ...')\n",
    "\n",
    "    ######## CHANGE THE PLAYER ########\n",
    "    best_player = Agent('best_player', env.state_size, env.action_size, config.MCTS_SIMS, config.CPUCT, best_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}